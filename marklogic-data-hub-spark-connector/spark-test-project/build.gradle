
plugins {
    id 'net.saliman.properties' version '1.5.1'
    id "java"
    id "com.marklogic.ml-gradle" version "4.1.0"
}

repositories {
    jcenter()
}

sourceCompatibility = "8"
targetCompatibility = "8"

ext {
    sparkVersion = "2.4.5"
}

dependencies {
    implementation fileTree(dir: "lib", includes: ["*.jar"])
    implementation "org.apache.spark:spark-sql_2.11:${sparkVersion}"
    implementation "org.apache.spark:spark-launcher_2.11:${sparkVersion}"
    implementation "org.apache.spark:spark-catalyst_2.11:${sparkVersion}"
    implementation "org.apache.spark:spark-streaming_2.11:${sparkVersion}"
    implementation "org.apache.spark:spark-core_2.11:${sparkVersion}"
}

task deployTDE() {
    def client = com.marklogic.client.DatabaseClientFactory.newClient("localhost", 8000,
            new com.marklogic.client.DatabaseClientFactory.DigestAuthContext(deployTDEUsername, deployTDEPassword));
    try {
        def demoTdeJavaScript = 'let tde = {\n' +
                '  "template":{\n' +
                '    "description":"databook tde",\n' +
                '    "context":"/envelope/instance",\n' +
                '    "collections":[\n' +
                '      "dataBook",\n' +
                '    ],\n' +
                '    "rows":[\n' +
                '      {\n' +
                '        "schemaName":"schema",\n' +
                '        "viewName":"dataBook",\n' +
                '        "viewLayout":"sparse",\n' +
                '        "columns":[\n' +
                '          {\n' +
                '            "name":"FirstName",\n' +
                '            "scalarType":"string",\n' +
                '            "val":"FirstName",\n' +
                '            "nullable":false, \n' +
                '            "invalidValues":"ignore"\n' +
                '          },\n' +
                '          {\n' +
                '            "name":"LastName",\n' +
                '            "scalarType":"string",\n' +
                '            "val":"FirstName",\n' +
                '            "nullable":true,\n' +
                '            "invalidValues":"ignore"\n' +
                '          },\n' +
                '          {\n' +
                '            "name":"Details",\n' +
                '            "scalarType":"string",\n' +
                '            "val":"Details",\n' +
                '            "nullable":true,\n' +
                '            "invalidValues":"ignore"\n' +
                '          }\n' +
                '          ,\n' +
                '          {\n' +
                '            "name":"Cell",\n' +
                '            "scalarType":"string",\n' +
                '            "val":"Cell",\n' +
                '            "nullable":true,\n' +
                '            "invalidValues":"ignore"\n' +
                '          }\n' +
                '        ] \n' +
                '      }\n' +
                '      ]\n' +
                '    }\n' +
                '}\n' +
                'xdmp.invokeFunction(function() {\n' +
                '    xdmp.documentInsert("/tde-dataBook.json",tde,{\n' +
                '      collections: [ "http://marklogic.com/xdmp/tde"],\n' +
                '      permissions: [ xdmp.permission("data-hub-operator","read"),xdmp.permission("data-hub-operator","update")]})\n' +
                '},{update:"true",\n' +
                '   database : xdmp.database("data-hub-staging-SCHEMAS")})\n' +
                '"TDEDeployed"'
            def result = client.newServerEval().javascript(demoTdeJavaScript).evalAs(String.class)
            if ( result.equals("TDEDeployed")) {
                println("TDE deployed. We are going to sleep for 10 seconds to finish reindexing.")
                sleep(10*1000)
            } else {
                throw new GradleException("Error during deploying TDE. Check the server log")
            }
    } finally {
        client.release()
    }
}


task queryDatabook(type: JavaExec) {
 //   dependsOn deployTDE()
    description = "Demo to read data from DataHub"
    classpath = sourceSets.main.runtimeClasspath
    main = "test.QueryDatabook"
    args = [
            host,
            username,
            password
    ]
}

task ingestDatabook(type: JavaExec) {
    description = "Run the test program to ingest the databook.csv contents via our Spark connector"
    classpath = sourceSets.main.runtimeClasspath
    main = "test.WriteTest"
    args = [
        host,
        username,
        password
    ]
}

task ingestDatabookViaStreamTest(type: JavaExec) {
    description = "Run the test program to ingest the databook.csv contents via our Stream connector"
    classpath = sourceSets.main.runtimeClasspath
    main = "test.StreamingWriteTest"
    args = [
        host,
        username,
        password
    ]
}
