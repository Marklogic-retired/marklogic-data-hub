{"id":20908, "kind":"Bug", "createdAt":"2013-02-22T14:39:26.315152-08:00", "status":"Closed", "title":"Missing Document: Missing one document after local disk + db replication stress test taking forests in and out of flash-backup mode", "category":"Replication", "severity":"Critical", "priority":{"level":"5", "title":""}, "submittedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "assignTo":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}, "description":"Missing Document: Missing one document after local disk + db replication stress test taking forests in and out of flash-backup mode\r\n\r\nThis is to track missing document issue just witnessed in the stress test. \r\n\r\nIn this test, we do not have any cronjobs that trigger failover. however, there are cronjobs on the e-nodes to take Modules, Triggers and MedlineDB forests into flash-backup and out every hour.\r\n\r\nAfter the test is done, we see 11,299,103 documents in Medline database. One document is missing which is \"medline02n0006-599-102.xml\"\r\n\r\nErrorLog on E-node shows this:\r\n2013-02-21 20:00:40.093 Debug: Retrying TaskServerTask::handleModule 2602331100215187343 Update 21 because XDMP-FORESTNOT: Forest F2A-M not available: wait replication\r\n2013-02-21 20:00:40.094 Debug: Detected compatibility for database MedlineDB\r\n2013-02-21 20:00:40.101 Warning: TaskServer: Unable to find medline02n0006-599-102.xml\r\n\r\n\r\nWill attach ErrorLogs from all hosts.\r\n\r\nMaster:\r\nrh5-amd64-failover-1 [E-Node]\r\nrh5-intel64-21 [D-Node]\r\nrh5-intel64-22 [D-Node]\r\n\r\nReplica:\r\nrh5-amd64-failover-2 [E-Node]\r\nrh5-intel64-23 [D-Node]\r\nrh5-intel64-24 [D-Node]\r\n\r\n\r\n", "samplequery":"", "sampledata":"", "version":"5.0-nightly", "tofixin":"5.0-6", "fixedin":"5.0-6", "platform":"all", "memory":"", "processors":"", "note":"", "subscribers":[{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, {"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, {"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}], "attachments":[{"name":"Errorlogs from all hosts on Master Cluster", "uri":"root/support/bugtracking/attachments/20908/masterlogs-022213-144047.tar.gz"}, {"name":"ErrorLogs from all hosts on Replica Cluster", "uri":"root/support/bugtracking/attachments/20908/replicalogs-022213-144055.tar.gz"}], "relationships":[{"type":"", "to":""}], "clones":[21402, 21403], "cloneOf":"", "support":{"headline":"Document missing from replica database after transitioning forest into and out of flash backup mode", "supportDescription":"In very rare situations, transitioning forests in both a master and replica databases at the same time while the master database is under load, the replica may miss some journal frames.", "publishStatus":"Publish", "tickets":[], "customerImpact":{"level":"N/A", "title":""}, "workaround":""}, "tags":["Replication", "svempati"], "changeHistory":[{"time":"2013-02-22T14:39:26.315152-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{"assignTo":{"from":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "to":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}}}, "files":[], "show":true}, {"time":"2013-02-22T14:46:56.048769-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Will rerun the test with the following trace events ON:\r\n\r\nForest Ask\r\nForest Fragment Insert\r\nForest Fragment Remove\r\nForest Abort\r\nForest DoAbort\r\nForest Commit\r\nForest DoCommit\r\nForest DoInsert\r\nForest Prepare\r\nForest Distributed Commit\r\nForest State\r\nForest Replicate\r\nForest Replicate Fragment\r\nForest Replicate Frame\r\nForest State\r\n\r\n\r\nin addition to (already enabled):\r\nForest Label\r\nForest State\r\nForest Replicate\r\nDatabase Replicate"}, {"time":"2013-02-22T15:20:42.108184-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Restarted the test on MarkLogic-5.0-20130222 with all the above trace flags enabled"}, {"time":"2013-02-23T12:29:50.422378-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"In the latest run with all the trace events, I see 12 documents missing at the end of the test. I \"unable to find\" in the logs for only 3 of them:\r\n\r\n/var/opt/MarkLogic/Logs/ErrorLog_1.txt:2013-02-22 16:00:45.369 Warning: TaskServer: Unable to find medline02n0070-23846-102.xml\r\n/var/opt/MarkLogic/Logs/ErrorLog_1.txt:2013-02-22 23:51:24.807 Warning: TaskServer: Unable to find medline02n0356-23453-102.xml\r\n/var/opt/MarkLogic/Logs/ErrorLog.txt:2013-02-23 00:07:09.876 Warning: TaskServer: Unable to find medline02n0360-23096-102.xml\r\n\r\nErroLogs will be available shortly at : /project/engineering/qa/bug20908/\r\n\r\nCannot attach logs to the bug as ErrorLog from each D-node is about 35G."}, {"time":"2013-02-24T16:59:31.514403-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Due to IT constraints, moving the data out of /project/engineering\r\n\r\nErrorLogs will be found under: /project/archive1/common/bug20908"}, {"time":"2013-02-25T17:32:02.605416-08:00", "updatedBy":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Wayne, Any update on this?"}, {"time":"2013-02-25T21:23:52.842986-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"I just finished looking over the transaction for the 3rd missing document, and everything looks fine with that. It looks to have been committed normally and properly replicated.\r\n\r\nIt's also telling that there were no forest state transitions around the time that the document went missing.\r\n\r\n"}, {"time":"2013-02-26T06:49:38.280773-08:00", "updatedBy":{"username":"cjl", "name":"Christopher Lindblad", "email":"cjl@cerisent.com"}, "change":{}, "files":[], "show":true, "comment":"I don;t know whether this is the problem, but think participantCommit and xaComplete should do ensureOpen(true) instead of ensureOpen(false).\r\nIt's a better idea to finish at that point than to restart.\r\n"}, {"time":"2013-02-26T13:28:21.318547-08:00", "updatedBy":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"We are rerunning the test on 5.0-4 to figure out if this is a new regression."}, {"time":"2013-02-27T10:37:00.103557-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Was not able to reproduce the issue on 5.0-4"}, {"time":"2013-02-27T23:39:48.589551-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Same test passed with 5.0-nightly when ran on a different (bigger) cluster with 1 e-node and 3 d-nodes."}, {"time":"2013-02-28T08:13:09.662243-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Replicated the issue on my three node cluster last night without database replication. "}, {"time":"2013-02-28T12:31:27.216062-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"There are a couple things going on here, at least in my area.\r\n\r\n1. The flash backup script isn't waiting for the forests to be open in flash backup mode before taking them back out of flash backup mode. There needs to be some sort of wait step. This will be impacted by bug 20840 which tracks the fact that when in flash backup mode all forests stall in \"wait replication\" state.\r\n\r\n2. When we're in flash backup mode, we don't have a forest label and so we also don't have information about who the master should be. \r\n\r\n3. The replica forest is coming out of flash backup mode before the master, and since the master didn't have access to its label while in flash backup mode there is no record that it was the master prior to entering flash backup mode. At this point, the replica decides it should step up as the master.\r\n\r\nThe problem is that the master transitioned to flash backup mode slightly before the master did, and so the master has a few extra journal frames that the replica doesn't know about. When the replica decides to become master, those journal frames from the original master are essentially discarded.\r\n\r\nI'm in the process of checking the logs from Sundeep's previous runs to see if they exhibit the same scenario.\r\n\r\nIf this is what's going on, I don't expect JPMC to hit the issue because they're not running local-disk failover.\r\n\r\nSundeep, can you detach the local-disk failover replica forests from the master forests in each cluster and rerun the test on 5.0-5? I think the issue will not duplicate in that environment.\r\n\r\nIf we get a clean run with that setup, I think we can go ahead and release 5.0-5 since it won't affect JPMC and other customers are clamoring to get it. We can address bugs 20840 and 20908 after the fact and make a call whether we want to release them in 5.0-6 or 5.0-5.1."}, {"time":"2013-02-28T12:46:08.839713-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Okay, will start the test without local disk failover on the 3 node cluster now.\r\n\r\nAlso, the script just puts the forest in flash backup mode via admin api and waits for the api to return. There is no additional wait after that.\r\n\r\nlet $numstands := fn:count(xdmp:forest-status($forest)//*:stand)\r\n        let $config := if($numstands le 20)\r\n                        then\r\n                        admin:forest-set-updates-allowed($config,\r\n                       $forest, \"flash-backup\")\r\n                       else\r\n                        $config\r\n\r\n\r\nThen it checks whether the forests are in flash-backup mode or not (config wise):\r\n\r\nlet $forests := xdmp:database-forests(xdmp:database($db))\r\n        for $forest in $forests\r\n        let $status := admin:forest-get-updates-allowed($config,\r\n                       $forest)\r\n                       return\r\n        concat(\"Updates allowed status for forest \",xdmp:forest-name($forest),\" is \",$status,\"\r\n               \")\r\n\r\n\r\nThis information is printed to /tmp/flash_backup.log\r\n\r\n"}, {"time":"2013-02-28T13:05:48.531043-08:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"logs from 5.0-4 run where 4 documents are missing on 3-node cluster are backed up to: /project/archive1/common/bug20908/on_5.0-4/\r\n\r\n"}, {"time":"2013-02-28T14:44:13.566944-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"More analysis of one of the missing documents Sundeep saw:\r\n\r\n2013-02-22 16:00:45.369 Warning: TaskServer: Unable to find medline02n0070-23846-102.xml\r\n\r\nWe're experiencing a false failover, due to both master and replica forests restarting at about the same time. What's happening is that both the master and replica forests go into wait replication around 16:00:33. The master correctly transitions to open and brings the replica up to date very quckly. The replica transitions to sync replicating at 16:00:39.700.\r\n\r\nThe master then begins handling transactions and transmitting frames to the replica, but the replica hasn't yet received and processed a heartbeat from the master to update its notion of the master's state.\r\n\r\nA short while later at 16:00:40.031, the replica still perceives the master to be in wait replication state, sees that it has a later FSN than what the master is reporting, and so it decides it has more up to date information than the master and is therefore best qualified to become a master.\r\n\r\nIf the replica had perceived the master to be in the open state, it would not have stepped in as master.\r\n\r\nI think the simplest fix would be to have the replica refuse updates from a master it doesn't perceive to be in the open state.\r\n\r\nHere are the relevant log entries.\r\n\r\n2013-02-22 16:00:05.266 rh5-intel64-22 F2A-M open -> start closing\r\n2013-02-22 16:00:05.367 rh5-intel64-22 F2A-M start closing -> finish closing\r\n2013-02-22 16:00:07.622 rh5-intel64-22 F2A-M finish closing -> mounted\r\n2013-02-22 16:00:07.635 rh5-intel64-22 F2A-M mounted -> unmounted\r\n2013-02-22 16:00:12.286 rh5-intel64-22 F2A-M unmounted -> mounted\r\n2013-02-22 16:00:12.299 rh5-intel64-22 F2A-M mounted -> wait replication\r\n\r\n2013-02-22 16:00:31.405 rh5-intel64-21 F2A-R start closing -> finish closing\r\n2013-02-22 16:00:31.405 rh5-intel64-21 F2A-R wait replication -> start closing\r\n2013-02-22 16:00:31.407 rh5-intel64-21 F2A-R finish closing -> mounted\r\n2013-02-22 16:00:31.407 rh5-intel64-21 F2A-R mounted -> unmounted\r\n\r\n2013-02-22 16:00:31.427 rh5-intel64-22 F2A-M start closing -> finish closing\r\n2013-02-22 16:00:31.427 rh5-intel64-22 F2A-M wait replication -> start closing\r\n2013-02-22 16:00:31.430 rh5-intel64-22 F2A-M finish closing -> mounted\r\n2013-02-22 16:00:31.430 rh5-intel64-22 F2A-M mounted -> unmounted\r\n\r\n2013-02-22 16:00:33.311 rh5-intel64-21 F2A-R unmounted -> mounted\r\n2013-02-22 16:00:33.327 rh5-intel64-21 F2A-R mounted -> recovering\r\n2013-02-22 16:00:33.333 rh5-intel64-21 F2A-R recovering -> wait replication\r\n\r\n2013-02-22 16:00:33.339 rh5-intel64-22 F2A-M unmounted -> mounted\r\n2013-02-22 16:00:33.384 rh5-intel64-22 F2A-M mounted -> recovering\r\n2013-02-22 16:00:33.398 rh5-intel64-22 F2A-M recovering -> wait replication\r\n\r\n2013-02-22 16:00:37.362 rh5-intel64-22 F2A-M assuming the role of master with old precise time 13615743731848640\r\n2013-02-22 16:00:37.370 rh5-intel64-22 F2A-M wait replication -> open\r\n\r\n2013-02-22 16:00:39.500 rh5-intel64-21 F2A-R wait replication -> async replicating\r\n2013-02-22 16:00:39.700 rh5-intel64-21 F2A-R async replicating -> sync replicating\r\n\r\n2013-02-22 16:00:39.932 rh5-intel64-22 F2A-M replicating to F2A-R fsn=11749607, op=insert\r\n2013-02-22 16:00:39.936 rh5-intel64-21 F2A-R received replicated journal frame fsn=11749607 op=insert mfor=F2A-M mtim=13615743731848640, mfsn=11749607\r\n2013-02-22 16:00:39.936 rh5-intel64-21 F2A-R doInsert recoveryToken={fsn=11747751 mf=F2A-M mpt=13615743731848640 mfsn=11749607\r\n\r\n2013-02-22 16:00:40.031 rh5-intel64-21 F2A-R considering forest F2A-M as master: online, wait replication, F2A-M, 13615743731848640, 11749606, !sync\r\n2013-02-22 16:00:40.031 rh5-intel64-21 F2A-R considering forest F2A-R as master: online, sync replicating, F2A-M, 13615743731848640, 11749607, sync\r\n2013-02-22 16:00:40.031 rh5-intel64-21 F2A-R assuming the role of master with new precise time 13615776400310480\r\n\r\n2013-02-22 16:00:40.037 rh5-intel64-21 F2A-R sync replicating -> open\r\n2013-02-22 16:00:41.436 rh5-intel64-22 F2A-M open -> wait replication\r\n2013-02-22 16:00:41.019 rh5-intel64-22 F2A-M inserts medline02n0070-23846-102.xml\r\n2013-02-22 16:00:41.025 rh5-intel64-21 F2A-M replicates insert to F2A-R\r\n2013-02-22 16:00:41.355 rh5-intel64-22 F2A-M discovered forest F2A-R opened more recently.\r\n2013-02-22 16:00:47.081 rh5-intel64-22 F2A-M wait replication -> async replicating\r\n2013-02-22 16:01:00.760 rh5-intel64-21 xdmp:forest-restart((F2A-R))\r\n2013-02-22 16:01:00.760 rh5-intel64-21 F2A-R open -> start closing\r\n\r\nSo the first missing file is explained, and I have a proposed fix coded. Moving on to look at the second missing file in more detail now."}, {"time":"2013-02-28T15:52:00.848543-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Analysis of the second missing file:\r\n\r\n2013-02-22 23:51:24.807 Warning: TaskServer: Unable to find medline02n0356-23453-102.xml\r\n\r\nRelevant logs:\r\n\r\n2013-02-22 23:50:28.733 rh5-intel64-21 F1B-M inserts document\r\n2013-02-22 23:50:28.733 rh5-intel64-21 F1B-M replicates insert to F1B-R\r\n2013-02-22 23:50:28.740 rh5-intel64-22 F1B-R receives replicated frame and inserts document\r\n2013-02-22 23:50:28.811 rh5-intel64-21 F1B-M begins distributed commit and replicates to F1B-R\r\n2013-02-22 23:50:28.828 rh5-intel64-21 F1B-M commits at timestamp 13616058288300900\r\n2013-02-22 23:50:28.829 rh5-intel64-21 F1B-M distributed end\r\n\r\nI don't see anything suggestive of why this one didn't stick."}, {"time":"2013-02-28T16:14:17.955377-08:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Investigating 3rd missing document.\r\n\r\n2013-02-23 00:07:09.876 Warning: TaskServer: Unable to find medline02n0360-23096-102.xml\r\n\r\nThe transaction looks to have committed correctly. There were some hung messages and dropped XDQP connections around the time of the missing document. Looking back at the second missing document, I also see hang messages and dropped connections from the e-node around the time of the missing document.\r\n\r\nRelevant logs:\r\n\r\n2013-02-23 00:00:41.244 rh5-intel64-21 Info: [Event:id=Forest State] Forest F1B-M state changed from wait replication to open at 13616064412448250\r\n2013-02-23 00:00:43.791 rh5-intel64-22 Info: [Event:id=Forest State] Forest F1B-R state changed from wait replication to async replicating at 13616064437916460\r\n2013-02-23 00:00:44.696 rh5-intel64-22 Info: [Event:id=Forest State] Forest F1B-R state changed from async replicating to sync replicating at 13616064446961850\r\n\r\n2013-02-23 00:06:39.186 rh5-intel64-21 Info: [Event:id=Forest Fragment Insert] Inserting Forest fragment: name=F1B-M, uriKey=medline02n0360-23096-102.xml\r\n2013-02-23 00:06:39.187 rh5-intel64-21 Info: [Event:id=Forest Replicate Frame] Forest F1B-M replicating to F1B-R: {{{fsn=19322851, chksum=0x6d99a850, words=3486}, op=insert\r\n2013-02-23 00:06:39.193 rh5-intel64-22 Info: [Event:id=Forest Replicate Frame] Forest F1B-R received replicated journal frame {{{fsn=19322851, chksum=0x6d99a850, words=3486}, op=insert\r\n2013-02-23 00:06:39.193 rh5-intel64-22 Info: [Event:id=Forest DoInsert] Forest F1B-R doInsert recoveryToken={fsn=19324201, off=456465472, sk=11941118502731434905}\r\n\r\n2013-02-23 00:06:44.441 rh5-intel64-21 Committing Forest distributed transaction: name=F1A-M\r\n2013-02-23 00:06:44.441 rh5-intel64-21 Forest F1A-M replicating to F1A-R: {{{fsn=22071205, chksum=0x3c4ca6d0, words=33}, op=distributed-begin\r\n2013-02-23 00:06:44.442 rh5-intel64-21 Preparing Forest transaction: name=F1B-M, {sessionKey=S1, coordinatorKey=F1A-M, request=(akeys=(9480390499158737821/medline02n0360-23096-102.xml)\r\n2013-02-23 00:06:44.442 rh5-intel64-21 Committing Forest transaction: name=F1A-M\r\n2013-02-23 00:06:44.444 rh5-intel64-21 Forest F1A-M marking participant F1B-M in session S1 as acknowledged\r\n2013-02-23 00:06:44.444 rh5-intel64-21 Forest F1A-M replicating to F1A-R: {{{fsn=22071207, chksum=0xcdb06a10, words=23}, op=distributed-end\r\n\r\n2013-02-23 00:06:27.758 rh5-amd64-failover-1 Debug: Hung 18 sec\r\n2013-02-23 00:06:49.119 rh5-amd64-failover-1 Debug: Hung 10 sec\r\n\r\n2013-02-23 00:07:04.479 rh5-amd64-failover-1 Debug: Stopping XDQPClientConnection, server=rh5-intel64-21.marklogic.com, conn=172.18.8.30:57318, sendTicks=0, recvTicks=0, sends=17577294,\r\n recvs=15785304, sendBytes=30655316296, recvBytes=7035742044, avgRTTmsec=3\r\n2013-02-23 00:07:05.500 rh5-amd64-failover-1 Debug: Hung 16 sec\r\n2013-02-23 00:07:08.080 rh5-amd64-failover-1 Debug: Stopping XDQPClientConnection, server=rh5-intel64-21.marklogic.com, conn=172.18.8.30:57317-172.18.9.44:7999, sendTicks=0, recvTicks=0\r\n, sends=17582659, recvs=15785449, sendBytes=30673405392, recvBytes=7033077188, avgRTTmsec=3\r\n2013-02-23 00:07:08.090 rh5-amd64-failover-1 Debug: Stopping XDQPClientConnection, server=rh5-intel64-21.marklogic.com, conn=172.18.8.30:57321-172.18.9.44:7999, sendTicks=0, recvTicks=0\r\n, sends=17593058, recvs=15785216, sendBytes=30722923700, recvBytes=7035847932, avgRTTmsec=3\r\n2013-02-23 00:07:08.091 rh5-amd64-failover-1 Debug: XDQPHost::heartbeat: SVC-SOCSEND: Socket send error: send 172.18.8.30:44033: Broken pipe\r\n2013-02-23 00:07:08.091 rh5-amd64-failover-1 Debug: Retrying TaskServerTask::handleModule 15612387982564169250 Update 1 because XDMP-XDQPDISC: XDQP connection disconnected, server=rh5-intel64-21.marklogic.com\r\n2013-02-23 00:07:08.091 rh5-amd64-failover-1 Debug: Retrying TaskServerTask::handleModule 3163991441202422059 Query 1 because SVC-SOCSEND: Socket send error: send 172.18.8.30:44033: Broken pipe\r\n2013-02-23 00:07:08.092 rh5-amd64-failover-1 Debug: Retrying TaskServerTask::handleModule 13275092958405996372 Update 1 because XDMP-XDQPDISC: XDQP connection disconnected, server=rh5-intel64-21.marklogic.com\r\n2013-02-23 00:07:08.092 rh5-amd64-failover-1 Debug: Starting XDQPClientConnection, server=rh5-intel64-21.marklogic.com, conn=172.18.8.30:47489-172.18.9.44:7999\r\n2013-02-23 00:07:08.092 rh5-amd64-failover-1 Debug: Retrying TaskServerTask::handleModule 2139366270613850224 Update 1 because XDMP-XDQPDISC: XDQP connection disconnected, server=rh5-intel64-21.marklogic.com\r\n2013-02-23 00:07:08.095 rh5-amd64-failover-1 Debug: Starting XDQPClientConnection, server=rh5-intel64-21.marklogic.com, conn=172.18.8.30:47490-172.18.9.44:7999\r\n2013-02-23 00:07:09.722 rh5-amd64-failover-1 Debug: Retrying TaskServerTask::handleModule 11401211875576514704 Update 1 because XDMP-XDQPNOSESSION: No XDQP session, client=rh5-amd64-failover-1.marklogic.com, request=insert, session=17762653748996930103, target=2377812163104510618\r\n\r\n2013-02-23 00:07:09.876 rh5-amd64-failover-1 Warning: TaskServer: Unable to find medline02n0360-23096-102.xml\r\n\r\n\r\n"}, {"time":"2013-03-01T14:53:50.108908-08:00", "updatedBy":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Since this is not a regression we have decided to move this to 5.0-6"}, {"time":"2013-03-01T14:53:50.108908-08:00", "updatedBy":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "change":{"tofixin":{"from":"5.0-5", "to":"5.0-6"}}, "files":[], "show":true}, {"time":"2013-04-01T16:20:57.267125-07:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"138311", "paths":["xdmp/branches/b5_0/src/Forest.cpp"], "affectedBugs":[]}, "comment":"bug:20908 replica forest must refuse updates from master until it perceives the master to be open"}, {"time":"2013-04-01T16:48:24.004434-07:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{"status":{"from":"", "to":"Test"}, "assignTo":{"from":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "to":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}}}, "files":[], "show":true}, {"time":"2013-04-01T16:48:24.004434-07:00", "updatedBy":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "change":{"assignTo":{"from":{"username":"wfeick", "name":"Wayne Feick", "email":"wfeick@marklogic.com"}, "to":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}}}, "files":[], "show":true}, {"time":"2013-07-15T00:39:14.33075-07:00", "updatedBy":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "change":{"assignTo":{"from":{"username":"rpolasani", "name":"Raghu Polasani", "email":"raghu.polasani@marklogic.com"}, "to":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}}}, "files":[], "show":true}, {"time":"2014-03-24T08:58:49.743267-07:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{"status":{"from":"", "to":"Ship"}, "assignTo":{"from":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "to":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}}}, "files":[], "show":true, "comment":"Verified on MarkLogic-5.0-20140320.x86_64"}, {"time":"2014-03-24T11:56:51.498209-07:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{"status":{"from":"", "to":"Test"}, "assignTo":{"from":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "to":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}}}, "files":[], "show":true, "comment":"Moving it back to test. See bug 26355."}, {"time":"2014-03-24T11:56:51.498209-07:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{"assignTo":{"from":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "to":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}}}, "files":[], "show":true}, {"time":"2014-03-25T10:29:47.50748-07:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Blocked on this bug:\r\nBug 26355 -  Failover Stress: One of the forests attached to Master database is stuck in 'start closing' state and the forest ended up in Error State"}, {"time":"2014-03-27T09:25:03.169557-07:00", "updatedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "change":{"status":{"from":"", "to":"Ship"}, "assignTo":{"from":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "to":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}}}, "files":[], "show":true, "comment":"Verified on MarkLogic-5.0-20140326"}, {"time":"2014-04-09T16:28:02.497853-07:00", "updatedBy":{"username":"rpelton", "name":"Rick Pelton", "email":"rick.pelton@marklogic.com"}, "change":{"status":{"from":"", "to":"Closed"}, "assignTo":{"from":{"username":"rpelton", "name":"Rick Pelton", "email":"rick.pelton@marklogic.com"}, "to":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}}}, "files":[], "show":true, "comment":"ga"}], "updatedAt":"2014-04-09T16:28:02.497853-07:00", "fixedAt":"2014-03-24T11:56:51.498209-07:00", "fixedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "shippedAt":"2014-03-27T09:25:03.169557-07:00", "shippedBy":{"username":"svempati", "name":"Sundeep Vempati", "email":"sundeep.vempati@marklogic.com"}, "closedAt":"2014-04-09T16:28:02.497853-07:00", "closedBy":{"username":"rpelton", "name":"Rick Pelton", "email":"rick.pelton@marklogic.com"}, "renderDescriptionAs":"normal"}