{"id":20627, "kind":"Task", "createdAt":"2013-01-29T11:25:27.789763-08:00", "status":"Closed", "title":"[task]Lexicon match optimization", "category":"search", "severity":"Performance", "priority":{"level":"5", "title":""}, "days":null, "period":{"startDate":null, "endDate":null}, "submittedBy":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "assignTo":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}, "description":"At least for the codepoint lexicon on disk, use multiple upper and lower bounds for performing scans\r\n\r\nWhen matching \"abc*\" as a case-insensitive, diacritic-sensitive match, the bounds are \"ABC\" to \"abd\"\r\nSince 'Z' < 'a' this means we scan through all the uppercase words.\r\n\r\nIf we ran this with two sets of bounds instead ('ABC', 'Abd') and ('abc', 'abd') this would be much more efficient.\r\n\r\nFor diacritic-insensitive this is more complex to do in the general case, but we could do it for the ASCII range.\r\ne.g. case/diacritic insensitive 'abc*' has bounds ('ABC', 'Åḇḓ'). We could run this as ('ABC','Aḇḓ''), ('aBC','aḇḓ''), ('ÀBC,Åḇḓ')\r\n('ABC','Aḇḓ''), ('aBC','aḇḓ''), ('{BC,Åḇḓ') The latter is probably sufficient and doesn't entail developing any new\r\ntables.\r\n\r\nThe trick here is that this will work for match strings in the ASCII range.  For those outside the ASCII range, we can't\r\nguarantee this will work (e.g. Greek has diacritical variants between the upper and lower case ranges). \r\nThis is for the codepoint collation.\r\n\r\nUCA collations tend to keep variants better clumped (that's the point of them, after all) so this is less of an issue.\r\n\r\n", "samplequery":"", "sampledata":"", "version":"7.0-nightly", "tofixin":"7.0-ea2", "fixedin":"7.0-ea2", "platform":"all", "memory":"", "processors":"", "note":"", "subscribers":[{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, {"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, {"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}], "attachments":[], "parent":{"type":"Sub-task", "parentId":"20625", "taskOrRfe":"task"}, "includeInTaskList":false, "proceduralTasks":{"Requirements Task":[], "Functional Specification Task":[], "Test Specification Task":[], "Test Automation Task":[], "Documentation Task":[]}, "subTasks":[], "tags":["search", "mary"], "changeHistory":[{"time":"2013-01-29T11:25:27.789763-08:00", "updatedBy":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "change":{"assignTo":{"from":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "to":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}}}, "files":[], "show":true}, {"time":"2013-02-04T09:29:08.361777-08:00", "updatedBy":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "change":{"assignTo":{"from":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "to":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}}}, "files":[], "show":true}, {"time":"2013-02-04T15:06:33.754008-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"implementation idea:\r\ninclude a vector<LexiconArrayIntervalIterator> in WordMatchStringIterator, facilitating the possibility to match on more than one consecutive interval on one query.\r\nChanges will be made to several Matcher's such as CodepointCollation::LCMatcher to construct the right interval vector."}, {"time":"2013-02-04T17:07:02.241279-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130614", "paths":["xdmp/trunk/src/Collation.cpp", "xdmp/trunk/src/Query.cpp", "xdmp/trunk/src/Query.h", "xdmp/trunk/src/InMemoryReverseIndex.cpp", "xdmp/trunk/src/OnDiskRangeIndexes.cpp", "xdmp/trunk/src/PathConfig.h", "xdmp/trunk/src/SearchBuiltins.cpp", "xdmp/trunk/src/OnDiskReverseIndex.cpp", "xdmp/trunk/src/ReverseIndexImpl.h", "xdmp/trunk/src/ReverseIndex.cpp", "xdmp/trunk/src/Collation.h"], "affectedBugs":["19357"]}, "comment":"Bug:20627 optimization on case-insensitive, diacritic-sensitive wildcard expansion bound, Bug:19357 add reverse query annotation"}, {"time":"2013-02-04T17:16:01.0522-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"After commit 130614,  query\r\ncts:search(doc(),cts:word-query(\"aa*\",(\"case-insensitive\",\"diacritic-sensitive\")))\r\nwill be ran under the following two bounds:\r\nAA,Ab\r\naA,ab\r\nwith the same interval mode (boundary open- and closeness).\r\n\r\nNote to self, it might be of interest to show bound information in xdmp:plan wildcard expansion."}, {"time":"2013-02-04T18:03:01.534153-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"One more thought, as the interval actually gives how many lexicon words need to be matched, we could balance trade off between binary search and matching.\r\ne.g. in case of \"abc*\" example:\r\nit is divided into ('ABC', 'Abd') and ('aBC', 'abd') then\r\nif (# of word falling into  ('ABC', 'Abd')  > CERTAINNUM) further divide into ('ABC','ABd') and ('AbC','Abd')\r\nso on and so forth...\r\nuntil we reach a point where we think it is not worth it to break down the interval anymore."}, {"time":"2013-02-04T18:05:16.145373-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Mary's comment\r\n\r\nOn Fri, 01 Feb 2013 13:37:48 -0800, Fei Xue <Fei.Xue@marklogic.com> wrote:\r\n\r\n> Hi Mary,\r\n> Question for task 20627,\r\n> With the example abc*, bound ('ABC','abc') were narrowed down to\r\n> ('ABC','Abd') and ('abc','abd')\r\n> The former of which could be further narrowed down to ('ABC','ABd') \r\n> and\r\n> ('AbC','Abd') etc.\r\n>\r\n> Is this what this task is trying to do? Or doing the first step narrow \r\n> down is enough?\r\n\r\nThe first step is enough.  There is a cost/benefit tradeoff here:\r\nfor every new range, that is another pair of binary searches through the range index to find that range.\r\n\r\n//Mary\r\n"}, {"time":"2013-02-04T18:38:05.778321-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130626", "paths":["xdmp/trunk/src/OnDiskRangeIndexes.cpp"], "affectedBugs":[]}, "comment":"Bug:20627 minor fix"}, {"time":"2013-02-05T09:45:38.086261-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130682", "paths":["xdmp/trunk/src/Collation.h"], "affectedBugs":[]}, "comment":"Bug:20627 minor fix"}, {"time":"2013-02-05T15:10:07.774446-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130711", "paths":["xdmp/trunk/src/Collation.cpp", "xdmp/trunk/src/OnDiskRangeIndexes.cpp", "xdmp/trunk/src/InMemoryRangeIndexes.cpp", "xdmp/trunk/src/Collation.h"], "affectedBugs":[]}, "comment":"Bug:20627 LCDLMatcher case, split interval three-ways"}, {"time":"2013-02-05T15:41:23.369614-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130718", "paths":["xdmp/trunk/src/Collation.cpp"], "affectedBugs":[]}, "comment":"Bug:20627 minor fix LCDL"}, {"time":"2013-02-05T15:43:23.858427-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Thoughts:\r\n\r\nWhen breaking down intervals, could do optimization on binary search because we know all the small intervals will be within the original covering interval."}, {"time":"2013-02-06T10:12:21.618957-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"130814", "paths":["xdmp/trunk/src/Collation.cpp"], "affectedBugs":[]}, "comment":"Bug:20627 testing chars in ASCii range before breaking intervals"}, {"time":"2013-02-07T20:29:54.740099-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"More thoughts on this:\r\n\r\n1. it is usually desirable to completely break down interval for a single trailing wildcard. Because if we do so, we could avoid matching each string all together. \r\noverhead: at most 2^(# of char+1) binary search\r\n\r\n2. so as to customize stuff, we should also provide customer with knob regarding whether they would like a complete break down.\r\n\r\n3. we could also optimize on those already completely broken down intervals, marking if matching is needed so as to save work.\r\n\r\n4. the first step break down should always be done, then for each stand will balance between cost and benefit. plan will also keep the overall interval trace."}, {"time":"2013-02-22T10:03:09.642172-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "svn":{"repository":"/project/engsvn", "revision":"133077", "paths":["xdmp/trunk/src/RangeIndexes.cpp", "xdmp/trunk/src/Collation.cpp", "xdmp/trunk/src/QueryVal.h", "xdmp/trunk/src/OnDiskRangeIndexes.cpp", "xdmp/trunk/src/SearchBuiltins.cpp", "xdmp/trunk/src/GeospatialBuiltins.cpp", "xdmp/trunk/src/Collation.h"], "affectedBugs":["20804"]}, "comment":"Bug:20804 + Bug:20627 infrastructure"}, {"time":"2013-02-26T16:04:29.432545-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Cost-based break down\r\n1. will continue breaking down if there are more than 256 entries to scan \r\n2. will always break down the first character possible and it will occur in xdmp:plan as lower and upper-bound, rest will be hidden as it will differ stand to stand\r\n3.special care for optimization on z as it is the last one on list"}, {"time":"2013-02-26T17:10:20.341861-08:00", "updatedBy":{"username":"mary", "name":"Mary Holstege", "email":"mary@cerisent.com"}, "change":{}, "files":[], "show":true, "comment":"Rather than first character: it should be the first character that has variants."}, {"time":"2013-02-26T17:53:53.851529-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Got it.\r\nFurther considerations:\r\n1, it is easier for LCMatcher, theoretically lower and upper bound are of the same length. \r\nwill just iterate until there is a variant and break down as is.\r\n2. complication comes into place in LCDL as upper and lower bound differ in length. between '{' and diacritic variant.\r\nNeed to keep track of the first variant position and positions the unicode character occupy.\r\n\r\nconsidering not breaking down character with diacritic and onward assuming a small portion of lexicon resides there. Yet this might be problematic for other languages(?)\r\ne.g. 'ABC', 'Åḇḓ'\r\n=> ('ABC','Aḇḓ') ('aBC','aḇḓ') ('}BC','Åḇḓ') \r\nonly first two intervals will be further broken down if more than 256 entries to\r\nThis makes finding the breaking character easier: first position where two bounds differ."}, {"time":"2013-02-26T18:33:22.202743-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Above change done on local copy, will verify it with a some regression and new developed test cases.\r\ninteresting cases include:\r\n12abc* abc12* a12bc* a* z* 1* Å* abc}* ab}c* 12Åḇḓ* Åḇḓ12* 12abz* a12bz*\r\ncould potentially validate first level bounds on test harness as it is available from plan."}, {"time":"2013-02-27T18:11:51.784993-08:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{}, "files":[], "show":true, "comment":"Submitted By: Fei Xue\tFebruary 27, 2013 18:09 PT\r\nComment: \r\nBug:20267 1)breaking down first variant 2)cost-based optimization stand level.Did some testing on 1) but need bigger dataset to test 2) watching regression tmr\r\nSVN Info: \r\nRevision: 134009\r\nModified Files:\r\nxdmp/trunk/src/Collation.cpp\r\nxdmp/trunk/src/OnDiskRangeIndexes.cpp\r\nxdmp/trunk/src/XQuery.cpp\r\nxdmp/trunk/src/Collation.h\r\nOther Bugs Referenced by this Commit:"}, {"time":"2013-05-15T15:53:01.348841-07:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{"status":{"from":"", "to":"Test"}, "assignTo":{"from":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "to":{"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}}}, "files":[], "show":true, "comment":"Optimizing wildcard internals."}, {"time":"2013-05-15T15:53:01.348841-07:00", "updatedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "change":{"assignTo":{"from":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "to":{"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}}}, "files":[], "show":true}, {"time":"2013-06-28T12:09:11.821594-07:00", "updatedBy":{"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}, "change":{"status":{"from":null, "to":"Ship"}, "assignTo":{"from":{"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}, "to":{"username":"nobody", "name":"nobody nobody", "email":"nobody@marklogic.com"}}}, "files":[], "show":true, "comment":"Verified on build 7.0-20130624. Adding some tests with trailing wildcards and different lexicon-expansion types on Shakespeare data. Will open separate bugs if there are issues. Moving this implementation bug to 'ship'."}], "updatedAt":"2013-06-28T12:09:11.821594-07:00", "fixedAt":"2013-05-15T15:53:01.348841-07:00", "fixedBy":{"username":"fxue", "name":"Fei Xue", "email":"Fei.Xue@marklogic.com"}, "shippedAt":"2013-06-28T12:09:11.821594-07:00", "shippedBy":{"username":"msrinivasan", "name":"Mahalakshmi Srinivasan", "email":"mahalakshmi.srinivasan@marklogic.com"}}